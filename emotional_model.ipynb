{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emotional_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedhisham73/Face-Detection-Recognition-Based-Attendance-System/blob/master/emotional_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrBdVxN0sWWq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "496d4d55-bef9-4507-d207-7250eab462c6"
      },
      "source": [
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VqV7wDdsoTc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc12f3a2-f5d9-4d8a-9e9c-9d4f4aeef6f3"
      },
      "source": [
        "import sys, os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import np_utils\n",
        "# pd.set_option('display.max_rows', 500)\n",
        "# pd.set_option('display.max_columns', 500)\n",
        "# pd.set_option('display.width', 1000)\n",
        "\n",
        "df=pd.read_csv('/content/gdrive/My Drive/fer2013/fer2013.csv')\n",
        "\n",
        "# print(df.info())\n",
        "# print(df[\"Usage\"].value_counts())\n",
        "\n",
        "# print(df.head())\n",
        "X_train,train_y,X_test,test_y=[],[],[],[]\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    val=row['pixels'].split(\" \")\n",
        "    try:\n",
        "        if 'Training' in row['Usage']:\n",
        "           X_train.append(np.array(val,'float32'))\n",
        "           train_y.append(row['emotion'])\n",
        "        elif 'PublicTest' in row['Usage']:\n",
        "           X_test.append(np.array(val,'float32'))\n",
        "           test_y.append(row['emotion'])\n",
        "    except:\n",
        "        print(f\"error occured at index :{index} and row:{row}\")\n",
        "\n",
        "\n",
        "num_features = 64\n",
        "num_labels = 7\n",
        "batch_size = 64\n",
        "epochs = 200\n",
        "width, height = 48, 48\n",
        "\n",
        "\n",
        "X_train = np.array(X_train,'float32')\n",
        "train_y = np.array(train_y,'float32')\n",
        "X_test = np.array(X_test,'float32')\n",
        "test_y = np.array(test_y,'float32')\n",
        "\n",
        "train_y=np_utils.to_categorical(train_y, num_classes=num_labels)\n",
        "test_y=np_utils.to_categorical(test_y, num_classes=num_labels)\n",
        "\n",
        "#cannot produce\n",
        "#normalizing data between oand 1\n",
        "X_train -= np.mean(X_train, axis=0)\n",
        "X_train /= np.std(X_train, axis=0)\n",
        "\n",
        "X_test -= np.mean(X_test, axis=0)\n",
        "X_test /= np.std(X_test, axis=0)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
        "\n",
        "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n",
        "\n",
        "# print(f\"shape:{X_train.shape}\")\n",
        "##designing the cnn\n",
        "#1st convolution layer\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))\n",
        "model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#2nd convolution layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#3rd convolution layer\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "#fully connected neural networks\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(num_labels, activation='softmax'))\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "#Compliling the model\n",
        "model.compile(loss=categorical_crossentropy,\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Training the model\n",
        "model.fit(X_train, train_y,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, test_y),\n",
        "          shuffle=True)\n",
        "\n",
        "\n",
        "#Saving the  model to  use it later on\n",
        "fer_json = model.to_json()\n",
        "with open(\"fer_1.json\", \"w\") as json_file:\n",
        "    json_file.write(fer_json)\n",
        "model.save_weights(\"fer_1.h5\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 28709 samples, validate on 3589 samples\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "28709/28709 [==============================] - 31s 1ms/step - loss: 1.7413 - acc: 0.2856 - val_loss: 1.5834 - val_acc: 0.3798\n",
            "Epoch 2/200\n",
            "28709/28709 [==============================] - 22s 775us/step - loss: 1.5471 - acc: 0.3932 - val_loss: 1.4648 - val_acc: 0.4193\n",
            "Epoch 3/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 1.4388 - acc: 0.4379 - val_loss: 1.3415 - val_acc: 0.4865\n",
            "Epoch 4/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 1.3659 - acc: 0.4722 - val_loss: 1.2884 - val_acc: 0.5043\n",
            "Epoch 5/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 1.3222 - acc: 0.4918 - val_loss: 1.2809 - val_acc: 0.5004\n",
            "Epoch 6/200\n",
            "28709/28709 [==============================] - 22s 774us/step - loss: 1.2891 - acc: 0.5021 - val_loss: 1.2513 - val_acc: 0.5116\n",
            "Epoch 7/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 1.2589 - acc: 0.5174 - val_loss: 1.2188 - val_acc: 0.5247\n",
            "Epoch 8/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 1.2284 - acc: 0.5301 - val_loss: 1.2032 - val_acc: 0.5405\n",
            "Epoch 9/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 1.2080 - acc: 0.5369 - val_loss: 1.2145 - val_acc: 0.5389\n",
            "Epoch 10/200\n",
            "28709/28709 [==============================] - 22s 775us/step - loss: 1.1881 - acc: 0.5447 - val_loss: 1.2010 - val_acc: 0.5428\n",
            "Epoch 11/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 1.1632 - acc: 0.5541 - val_loss: 1.1870 - val_acc: 0.5520\n",
            "Epoch 12/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 1.1464 - acc: 0.5607 - val_loss: 1.1976 - val_acc: 0.5506\n",
            "Epoch 13/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 1.1270 - acc: 0.5714 - val_loss: 1.1661 - val_acc: 0.5531\n",
            "Epoch 14/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 1.1103 - acc: 0.5761 - val_loss: 1.1723 - val_acc: 0.5548\n",
            "Epoch 15/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 1.0950 - acc: 0.5815 - val_loss: 1.1692 - val_acc: 0.5642\n",
            "Epoch 16/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 1.0770 - acc: 0.5893 - val_loss: 1.1583 - val_acc: 0.5617\n",
            "Epoch 17/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 1.0665 - acc: 0.5922 - val_loss: 1.1810 - val_acc: 0.5511\n",
            "Epoch 18/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 1.0466 - acc: 0.6023 - val_loss: 1.1627 - val_acc: 0.5631\n",
            "Epoch 19/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 1.0229 - acc: 0.6118 - val_loss: 1.1888 - val_acc: 0.5587\n",
            "Epoch 20/200\n",
            "28709/28709 [==============================] - 22s 773us/step - loss: 1.0163 - acc: 0.6131 - val_loss: 1.1481 - val_acc: 0.5653\n",
            "Epoch 21/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 1.0027 - acc: 0.6188 - val_loss: 1.1927 - val_acc: 0.5486\n",
            "Epoch 22/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.9923 - acc: 0.6202 - val_loss: 1.1744 - val_acc: 0.5559\n",
            "Epoch 23/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.9784 - acc: 0.6267 - val_loss: 1.1982 - val_acc: 0.5603\n",
            "Epoch 24/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.9574 - acc: 0.6334 - val_loss: 1.1753 - val_acc: 0.5681\n",
            "Epoch 25/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.9447 - acc: 0.6400 - val_loss: 1.2016 - val_acc: 0.5626\n",
            "Epoch 26/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.9316 - acc: 0.6424 - val_loss: 1.1894 - val_acc: 0.5645\n",
            "Epoch 27/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.9152 - acc: 0.6521 - val_loss: 1.1892 - val_acc: 0.5659\n",
            "Epoch 28/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.9020 - acc: 0.6608 - val_loss: 1.2028 - val_acc: 0.5575\n",
            "Epoch 29/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.8931 - acc: 0.6611 - val_loss: 1.2115 - val_acc: 0.5687\n",
            "Epoch 30/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.8701 - acc: 0.6712 - val_loss: 1.2308 - val_acc: 0.5662\n",
            "Epoch 31/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.8699 - acc: 0.6683 - val_loss: 1.2224 - val_acc: 0.5520\n",
            "Epoch 32/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.8511 - acc: 0.6766 - val_loss: 1.2040 - val_acc: 0.5698\n",
            "Epoch 33/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.8404 - acc: 0.6863 - val_loss: 1.2207 - val_acc: 0.5695\n",
            "Epoch 34/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.8330 - acc: 0.6865 - val_loss: 1.2210 - val_acc: 0.5598\n",
            "Epoch 35/200\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 0.8119 - acc: 0.6941 - val_loss: 1.2570 - val_acc: 0.5617\n",
            "Epoch 36/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.8059 - acc: 0.6962 - val_loss: 1.2199 - val_acc: 0.5609\n",
            "Epoch 37/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.7970 - acc: 0.6991 - val_loss: 1.2379 - val_acc: 0.5690\n",
            "Epoch 38/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.7882 - acc: 0.7054 - val_loss: 1.2483 - val_acc: 0.5715\n",
            "Epoch 39/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.7726 - acc: 0.7090 - val_loss: 1.2538 - val_acc: 0.5734\n",
            "Epoch 40/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.7724 - acc: 0.7125 - val_loss: 1.2360 - val_acc: 0.5715\n",
            "Epoch 41/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.7607 - acc: 0.7135 - val_loss: 1.2519 - val_acc: 0.5740\n",
            "Epoch 42/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.7399 - acc: 0.7209 - val_loss: 1.2670 - val_acc: 0.5701\n",
            "Epoch 43/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.7347 - acc: 0.7229 - val_loss: 1.2522 - val_acc: 0.5729\n",
            "Epoch 44/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.7271 - acc: 0.7272 - val_loss: 1.2819 - val_acc: 0.5751\n",
            "Epoch 45/200\n",
            "28709/28709 [==============================] - 22s 773us/step - loss: 0.7159 - acc: 0.7320 - val_loss: 1.2863 - val_acc: 0.5740\n",
            "Epoch 46/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.7123 - acc: 0.7347 - val_loss: 1.2839 - val_acc: 0.5726\n",
            "Epoch 47/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.7000 - acc: 0.7390 - val_loss: 1.3130 - val_acc: 0.5670\n",
            "Epoch 48/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.6845 - acc: 0.7474 - val_loss: 1.3443 - val_acc: 0.5701\n",
            "Epoch 49/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.6882 - acc: 0.7444 - val_loss: 1.2997 - val_acc: 0.5743\n",
            "Epoch 50/200\n",
            "28709/28709 [==============================] - 22s 773us/step - loss: 0.6654 - acc: 0.7507 - val_loss: 1.3608 - val_acc: 0.5639\n",
            "Epoch 51/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.6644 - acc: 0.7504 - val_loss: 1.3329 - val_acc: 0.5737\n",
            "Epoch 52/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.6648 - acc: 0.7514 - val_loss: 1.3298 - val_acc: 0.5665\n",
            "Epoch 53/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.6558 - acc: 0.7584 - val_loss: 1.3298 - val_acc: 0.5832\n",
            "Epoch 54/200\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 0.6459 - acc: 0.7614 - val_loss: 1.3358 - val_acc: 0.5851\n",
            "Epoch 55/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.6344 - acc: 0.7633 - val_loss: 1.3551 - val_acc: 0.5759\n",
            "Epoch 56/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.6213 - acc: 0.7694 - val_loss: 1.3450 - val_acc: 0.5793\n",
            "Epoch 57/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.6364 - acc: 0.7652 - val_loss: 1.3233 - val_acc: 0.5770\n",
            "Epoch 58/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.6146 - acc: 0.7718 - val_loss: 1.3671 - val_acc: 0.5773\n",
            "Epoch 59/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.6131 - acc: 0.7720 - val_loss: 1.3829 - val_acc: 0.5729\n",
            "Epoch 60/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.6069 - acc: 0.7766 - val_loss: 1.3679 - val_acc: 0.5748\n",
            "Epoch 61/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.6032 - acc: 0.7802 - val_loss: 1.3841 - val_acc: 0.5712\n",
            "Epoch 62/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.6040 - acc: 0.7776 - val_loss: 1.4135 - val_acc: 0.5704\n",
            "Epoch 63/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.5852 - acc: 0.7831 - val_loss: 1.4620 - val_acc: 0.5626\n",
            "Epoch 64/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.5836 - acc: 0.7857 - val_loss: 1.4119 - val_acc: 0.5687\n",
            "Epoch 65/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.5808 - acc: 0.7888 - val_loss: 1.4178 - val_acc: 0.5678\n",
            "Epoch 66/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.5663 - acc: 0.7929 - val_loss: 1.4697 - val_acc: 0.5717\n",
            "Epoch 67/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.5637 - acc: 0.7948 - val_loss: 1.4537 - val_acc: 0.5681\n",
            "Epoch 68/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.5622 - acc: 0.7927 - val_loss: 1.4627 - val_acc: 0.5678\n",
            "Epoch 69/200\n",
            "28709/28709 [==============================] - 22s 774us/step - loss: 0.5583 - acc: 0.7967 - val_loss: 1.4714 - val_acc: 0.5809\n",
            "Epoch 70/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.5631 - acc: 0.7949 - val_loss: 1.4753 - val_acc: 0.5704\n",
            "Epoch 71/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.5492 - acc: 0.8006 - val_loss: 1.4347 - val_acc: 0.5762\n",
            "Epoch 72/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.5394 - acc: 0.8036 - val_loss: 1.4494 - val_acc: 0.5773\n",
            "Epoch 73/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.5349 - acc: 0.8045 - val_loss: 1.5013 - val_acc: 0.5731\n",
            "Epoch 74/200\n",
            "28709/28709 [==============================] - 22s 779us/step - loss: 0.5371 - acc: 0.8043 - val_loss: 1.4389 - val_acc: 0.5701\n",
            "Epoch 75/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.5270 - acc: 0.8074 - val_loss: 1.4658 - val_acc: 0.5737\n",
            "Epoch 76/200\n",
            "28709/28709 [==============================] - 22s 773us/step - loss: 0.5222 - acc: 0.8105 - val_loss: 1.4906 - val_acc: 0.5709\n",
            "Epoch 77/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.5198 - acc: 0.8111 - val_loss: 1.4739 - val_acc: 0.5754\n",
            "Epoch 78/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.5050 - acc: 0.8171 - val_loss: 1.5417 - val_acc: 0.5779\n",
            "Epoch 79/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.5116 - acc: 0.8180 - val_loss: 1.5231 - val_acc: 0.5681\n",
            "Epoch 80/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.5130 - acc: 0.8146 - val_loss: 1.5256 - val_acc: 0.5642\n",
            "Epoch 81/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.5067 - acc: 0.8149 - val_loss: 1.5312 - val_acc: 0.5743\n",
            "Epoch 82/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.4938 - acc: 0.8230 - val_loss: 1.4727 - val_acc: 0.5715\n",
            "Epoch 83/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.4931 - acc: 0.8228 - val_loss: 1.4863 - val_acc: 0.5759\n",
            "Epoch 84/200\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 0.4816 - acc: 0.8263 - val_loss: 1.5917 - val_acc: 0.5684\n",
            "Epoch 85/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.4840 - acc: 0.8258 - val_loss: 1.5175 - val_acc: 0.5637\n",
            "Epoch 86/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.4907 - acc: 0.8214 - val_loss: 1.5528 - val_acc: 0.5545\n",
            "Epoch 87/200\n",
            "28709/28709 [==============================] - 22s 773us/step - loss: 0.4899 - acc: 0.8245 - val_loss: 1.5580 - val_acc: 0.5720\n",
            "Epoch 88/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.4742 - acc: 0.8291 - val_loss: 1.5512 - val_acc: 0.5667\n",
            "Epoch 89/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.4719 - acc: 0.8312 - val_loss: 1.5381 - val_acc: 0.5776\n",
            "Epoch 90/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.4709 - acc: 0.8320 - val_loss: 1.5988 - val_acc: 0.5592\n",
            "Epoch 91/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.4663 - acc: 0.8347 - val_loss: 1.5775 - val_acc: 0.5659\n",
            "Epoch 92/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.4637 - acc: 0.8336 - val_loss: 1.5570 - val_acc: 0.5740\n",
            "Epoch 93/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.4548 - acc: 0.8380 - val_loss: 1.6020 - val_acc: 0.5726\n",
            "Epoch 94/200\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 0.4495 - acc: 0.8406 - val_loss: 1.5911 - val_acc: 0.5779\n",
            "Epoch 95/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.4444 - acc: 0.8402 - val_loss: 1.5903 - val_acc: 0.5712\n",
            "Epoch 96/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.4472 - acc: 0.8434 - val_loss: 1.6503 - val_acc: 0.5773\n",
            "Epoch 97/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.4396 - acc: 0.8459 - val_loss: 1.5979 - val_acc: 0.5798\n",
            "Epoch 98/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.4462 - acc: 0.8423 - val_loss: 1.5944 - val_acc: 0.5659\n",
            "Epoch 99/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.4475 - acc: 0.8417 - val_loss: 1.6247 - val_acc: 0.5751\n",
            "Epoch 100/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.4396 - acc: 0.8445 - val_loss: 1.6139 - val_acc: 0.5754\n",
            "Epoch 101/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.4418 - acc: 0.8447 - val_loss: 1.6202 - val_acc: 0.5773\n",
            "Epoch 102/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.4321 - acc: 0.8468 - val_loss: 1.5722 - val_acc: 0.5695\n",
            "Epoch 103/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.4247 - acc: 0.8499 - val_loss: 1.6770 - val_acc: 0.5762\n",
            "Epoch 104/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.4254 - acc: 0.8472 - val_loss: 1.6338 - val_acc: 0.5706\n",
            "Epoch 105/200\n",
            "28709/28709 [==============================] - 22s 773us/step - loss: 0.4135 - acc: 0.8543 - val_loss: 1.6132 - val_acc: 0.5626\n",
            "Epoch 106/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.4116 - acc: 0.8546 - val_loss: 1.6618 - val_acc: 0.5737\n",
            "Epoch 107/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.4281 - acc: 0.8510 - val_loss: 1.6277 - val_acc: 0.5790\n",
            "Epoch 108/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.4224 - acc: 0.8519 - val_loss: 1.6374 - val_acc: 0.5715\n",
            "Epoch 109/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.4136 - acc: 0.8561 - val_loss: 1.5666 - val_acc: 0.5678\n",
            "Epoch 110/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.4072 - acc: 0.8546 - val_loss: 1.6992 - val_acc: 0.5723\n",
            "Epoch 111/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.4155 - acc: 0.8536 - val_loss: 1.6004 - val_acc: 0.5768\n",
            "Epoch 112/200\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 0.3969 - acc: 0.8605 - val_loss: 1.6986 - val_acc: 0.5765\n",
            "Epoch 113/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.4013 - acc: 0.8580 - val_loss: 1.6650 - val_acc: 0.5678\n",
            "Epoch 114/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.3990 - acc: 0.8582 - val_loss: 1.6270 - val_acc: 0.5815\n",
            "Epoch 115/200\n",
            "28709/28709 [==============================] - 22s 773us/step - loss: 0.3959 - acc: 0.8634 - val_loss: 1.7101 - val_acc: 0.5676\n",
            "Epoch 116/200\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 0.4123 - acc: 0.8557 - val_loss: 1.7112 - val_acc: 0.5678\n",
            "Epoch 117/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.4014 - acc: 0.8604 - val_loss: 1.6525 - val_acc: 0.5687\n",
            "Epoch 118/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.3991 - acc: 0.8585 - val_loss: 1.6118 - val_acc: 0.5754\n",
            "Epoch 119/200\n",
            "28709/28709 [==============================] - 22s 765us/step - loss: 0.3953 - acc: 0.8609 - val_loss: 1.7694 - val_acc: 0.5653\n",
            "Epoch 120/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.3945 - acc: 0.8616 - val_loss: 1.6492 - val_acc: 0.5759\n",
            "Epoch 121/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.3787 - acc: 0.8675 - val_loss: 1.6925 - val_acc: 0.5706\n",
            "Epoch 122/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.4031 - acc: 0.8603 - val_loss: 1.6986 - val_acc: 0.5726\n",
            "Epoch 123/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.3931 - acc: 0.8651 - val_loss: 1.5830 - val_acc: 0.5681\n",
            "Epoch 124/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.3911 - acc: 0.8633 - val_loss: 1.6957 - val_acc: 0.5690\n",
            "Epoch 125/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3837 - acc: 0.8689 - val_loss: 1.7189 - val_acc: 0.5731\n",
            "Epoch 126/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3850 - acc: 0.8660 - val_loss: 1.6044 - val_acc: 0.5690\n",
            "Epoch 127/200\n",
            "28709/28709 [==============================] - 22s 766us/step - loss: 0.3774 - acc: 0.8702 - val_loss: 1.7219 - val_acc: 0.5681\n",
            "Epoch 128/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.3922 - acc: 0.8608 - val_loss: 1.6618 - val_acc: 0.5768\n",
            "Epoch 129/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3794 - acc: 0.8700 - val_loss: 1.7107 - val_acc: 0.5653\n",
            "Epoch 130/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.3767 - acc: 0.8695 - val_loss: 1.7010 - val_acc: 0.5648\n",
            "Epoch 131/200\n",
            "28709/28709 [==============================] - 22s 773us/step - loss: 0.3597 - acc: 0.8755 - val_loss: 1.7137 - val_acc: 0.5756\n",
            "Epoch 132/200\n",
            "28709/28709 [==============================] - 22s 775us/step - loss: 0.3710 - acc: 0.8727 - val_loss: 1.6961 - val_acc: 0.5818\n",
            "Epoch 133/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.3740 - acc: 0.8696 - val_loss: 1.6840 - val_acc: 0.5731\n",
            "Epoch 134/200\n",
            "28709/28709 [==============================] - 22s 773us/step - loss: 0.3749 - acc: 0.8717 - val_loss: 1.6992 - val_acc: 0.5626\n",
            "Epoch 135/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.3559 - acc: 0.8775 - val_loss: 1.7863 - val_acc: 0.5726\n",
            "Epoch 136/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.3701 - acc: 0.8730 - val_loss: 1.7786 - val_acc: 0.5717\n",
            "Epoch 137/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3611 - acc: 0.8750 - val_loss: 1.7481 - val_acc: 0.5642\n",
            "Epoch 138/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.3614 - acc: 0.8767 - val_loss: 1.8208 - val_acc: 0.5737\n",
            "Epoch 139/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3583 - acc: 0.8752 - val_loss: 1.7811 - val_acc: 0.5690\n",
            "Epoch 140/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3486 - acc: 0.8791 - val_loss: 1.8080 - val_acc: 0.5665\n",
            "Epoch 141/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.3528 - acc: 0.8799 - val_loss: 1.7259 - val_acc: 0.5698\n",
            "Epoch 142/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.3608 - acc: 0.8775 - val_loss: 1.7734 - val_acc: 0.5770\n",
            "Epoch 143/200\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 0.3532 - acc: 0.8787 - val_loss: 1.7817 - val_acc: 0.5709\n",
            "Epoch 144/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3515 - acc: 0.8793 - val_loss: 1.7772 - val_acc: 0.5745\n",
            "Epoch 145/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3612 - acc: 0.8761 - val_loss: 1.7156 - val_acc: 0.5701\n",
            "Epoch 146/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.3615 - acc: 0.8756 - val_loss: 1.7698 - val_acc: 0.5720\n",
            "Epoch 147/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3558 - acc: 0.8780 - val_loss: 1.7725 - val_acc: 0.5790\n",
            "Epoch 148/200\n",
            "28709/28709 [==============================] - 22s 773us/step - loss: 0.3567 - acc: 0.8767 - val_loss: 1.8858 - val_acc: 0.5790\n",
            "Epoch 149/200\n",
            "28709/28709 [==============================] - 22s 773us/step - loss: 0.3533 - acc: 0.8783 - val_loss: 1.7687 - val_acc: 0.5729\n",
            "Epoch 150/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.3562 - acc: 0.8766 - val_loss: 1.7470 - val_acc: 0.5779\n",
            "Epoch 151/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.3581 - acc: 0.8793 - val_loss: 1.7122 - val_acc: 0.5687\n",
            "Epoch 152/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.3410 - acc: 0.8824 - val_loss: 1.7614 - val_acc: 0.5765\n",
            "Epoch 153/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.3461 - acc: 0.8831 - val_loss: 1.7595 - val_acc: 0.5762\n",
            "Epoch 154/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.3676 - acc: 0.8729 - val_loss: 1.7685 - val_acc: 0.5731\n",
            "Epoch 155/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.3374 - acc: 0.8840 - val_loss: 1.7351 - val_acc: 0.5637\n",
            "Epoch 156/200\n",
            "28709/28709 [==============================] - 22s 772us/step - loss: 0.3476 - acc: 0.8809 - val_loss: 1.7472 - val_acc: 0.5678\n",
            "Epoch 157/200\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 0.3461 - acc: 0.8828 - val_loss: 1.7896 - val_acc: 0.5748\n",
            "Epoch 158/200\n",
            "28709/28709 [==============================] - 22s 771us/step - loss: 0.3407 - acc: 0.8822 - val_loss: 1.6935 - val_acc: 0.5793\n",
            "Epoch 159/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.3376 - acc: 0.8853 - val_loss: 1.7931 - val_acc: 0.5798\n",
            "Epoch 160/200\n",
            "28709/28709 [==============================] - 22s 773us/step - loss: 0.3397 - acc: 0.8844 - val_loss: 1.8083 - val_acc: 0.5759\n",
            "Epoch 161/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3400 - acc: 0.8854 - val_loss: 1.8736 - val_acc: 0.5603\n",
            "Epoch 162/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3411 - acc: 0.8837 - val_loss: 1.6963 - val_acc: 0.5704\n",
            "Epoch 163/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.3365 - acc: 0.8858 - val_loss: 1.7102 - val_acc: 0.5701\n",
            "Epoch 164/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3300 - acc: 0.8864 - val_loss: 1.8333 - val_acc: 0.5762\n",
            "Epoch 165/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3251 - acc: 0.8877 - val_loss: 1.8278 - val_acc: 0.5743\n",
            "Epoch 166/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3281 - acc: 0.8886 - val_loss: 1.8159 - val_acc: 0.5690\n",
            "Epoch 167/200\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 0.3419 - acc: 0.8875 - val_loss: 1.8096 - val_acc: 0.5612\n",
            "Epoch 168/200\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 0.3428 - acc: 0.8838 - val_loss: 1.8221 - val_acc: 0.5626\n",
            "Epoch 169/200\n",
            "28709/28709 [==============================] - 22s 765us/step - loss: 0.3368 - acc: 0.8857 - val_loss: 1.8675 - val_acc: 0.5570\n",
            "Epoch 170/200\n",
            "28709/28709 [==============================] - 22s 765us/step - loss: 0.3242 - acc: 0.8882 - val_loss: 1.8230 - val_acc: 0.5670\n",
            "Epoch 171/200\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 0.3304 - acc: 0.8895 - val_loss: 1.7889 - val_acc: 0.5734\n",
            "Epoch 172/200\n",
            "28709/28709 [==============================] - 22s 766us/step - loss: 0.3227 - acc: 0.8902 - val_loss: 1.7848 - val_acc: 0.5715\n",
            "Epoch 173/200\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 0.3271 - acc: 0.8918 - val_loss: 1.7692 - val_acc: 0.5773\n",
            "Epoch 174/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3287 - acc: 0.8874 - val_loss: 1.7691 - val_acc: 0.5832\n",
            "Epoch 175/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.3266 - acc: 0.8888 - val_loss: 1.8543 - val_acc: 0.5712\n",
            "Epoch 176/200\n",
            "28709/28709 [==============================] - 22s 765us/step - loss: 0.3225 - acc: 0.8925 - val_loss: 1.7661 - val_acc: 0.5720\n",
            "Epoch 177/200\n",
            "28709/28709 [==============================] - 22s 766us/step - loss: 0.3172 - acc: 0.8928 - val_loss: 1.7958 - val_acc: 0.5698\n",
            "Epoch 178/200\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 0.3283 - acc: 0.8882 - val_loss: 1.9133 - val_acc: 0.5717\n",
            "Epoch 179/200\n",
            "28709/28709 [==============================] - 22s 765us/step - loss: 0.3320 - acc: 0.8881 - val_loss: 1.8240 - val_acc: 0.5706\n",
            "Epoch 180/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.3199 - acc: 0.8902 - val_loss: 1.8305 - val_acc: 0.5743\n",
            "Epoch 181/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3257 - acc: 0.8922 - val_loss: 1.8588 - val_acc: 0.5665\n",
            "Epoch 182/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3131 - acc: 0.8945 - val_loss: 1.8177 - val_acc: 0.5659\n",
            "Epoch 183/200\n",
            "28709/28709 [==============================] - 22s 766us/step - loss: 0.3273 - acc: 0.8900 - val_loss: 1.7779 - val_acc: 0.5600\n",
            "Epoch 184/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3154 - acc: 0.8928 - val_loss: 1.8858 - val_acc: 0.5581\n",
            "Epoch 185/200\n",
            "28709/28709 [==============================] - 22s 766us/step - loss: 0.3194 - acc: 0.8923 - val_loss: 1.8082 - val_acc: 0.5692\n",
            "Epoch 186/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3278 - acc: 0.8879 - val_loss: 1.8020 - val_acc: 0.5659\n",
            "Epoch 187/200\n",
            "28709/28709 [==============================] - 22s 766us/step - loss: 0.3081 - acc: 0.8969 - val_loss: 1.9752 - val_acc: 0.5626\n",
            "Epoch 188/200\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 0.3332 - acc: 0.8900 - val_loss: 1.7569 - val_acc: 0.5701\n",
            "Epoch 189/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.2998 - acc: 0.9005 - val_loss: 1.8936 - val_acc: 0.5787\n",
            "Epoch 190/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.3149 - acc: 0.8946 - val_loss: 1.8384 - val_acc: 0.5712\n",
            "Epoch 191/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3162 - acc: 0.8936 - val_loss: 1.8636 - val_acc: 0.5612\n",
            "Epoch 192/200\n",
            "28709/28709 [==============================] - 22s 766us/step - loss: 0.3273 - acc: 0.8913 - val_loss: 1.8835 - val_acc: 0.5595\n",
            "Epoch 193/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3053 - acc: 0.8996 - val_loss: 1.9466 - val_acc: 0.5720\n",
            "Epoch 194/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3308 - acc: 0.8911 - val_loss: 1.8755 - val_acc: 0.5595\n",
            "Epoch 195/200\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 0.3121 - acc: 0.8950 - val_loss: 1.8959 - val_acc: 0.5695\n",
            "Epoch 196/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3153 - acc: 0.8940 - val_loss: 1.9635 - val_acc: 0.5587\n",
            "Epoch 197/200\n",
            "28709/28709 [==============================] - 22s 769us/step - loss: 0.3068 - acc: 0.8970 - val_loss: 1.8476 - val_acc: 0.5723\n",
            "Epoch 198/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.3136 - acc: 0.8948 - val_loss: 1.8900 - val_acc: 0.5751\n",
            "Epoch 199/200\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.3033 - acc: 0.8995 - val_loss: 1.9186 - val_acc: 0.5665\n",
            "Epoch 200/200\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 0.3072 - acc: 0.9008 - val_loss: 1.9141 - val_acc: 0.5684\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}